{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# ReasonForge\n",
        "\n",
        "Deterministic math & code tools for small language models.\n",
        "\n",
        "1. Clone repo & install deps\n",
        "2. Install Ollama & pull model\n",
        "3. Sanity tests\n",
        "4. MATH-500 / HumanEval benchmarks\n",
        "5. Gradio chat UI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_setup"
      },
      "source": [
        "---\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone_and_install"
      },
      "outputs": [],
      "source": [
        "# Clone repo & install deps\n",
        "!git clone https://github.com/RoyCoding8/MCP.git /content/MCP 2>/dev/null || echo 'Already cloned'\n",
        "!pip install -q \"requests>=2.31.0\" \"gradio>=6.0\" \"sympy>=1.13.0\" \"datasets>=4.6.1\" \"math-verify[antlr4_13_2]>=0.9.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_ollama"
      },
      "outputs": [],
      "source": [
        "# Install Ollama\n",
        "!sudo apt-get install -qq zstd\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount_drive"
      },
      "outputs": [],
      "source": [
        "# Set USE_DRIVE=True to cache models on Google Drive across sessions\n",
        "import os\n",
        "USE_DRIVE = True\n",
        "\n",
        "if USE_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    MODELS_PATH = '/content/drive/MyDrive/ollama_models'\n",
        "else:\n",
        "    MODELS_PATH = '/content/ollama_models'\n",
        "\n",
        "os.makedirs(MODELS_PATH, exist_ok=True)\n",
        "print(f'Models: {MODELS_PATH}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "start_ollama"
      },
      "outputs": [],
      "source": [
        "import subprocess, time, threading, requests\n",
        "\n",
        "subprocess.run(['pkill', '-f', 'ollama'], capture_output=True)\n",
        "time.sleep(2)\n",
        "\n",
        "env = os.environ.copy()\n",
        "env['OLLAMA_MODELS'] = MODELS_PATH\n",
        "env['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "\n",
        "log_file = open('/content/ollama_server.log', 'w')\n",
        "proc = subprocess.Popen(\n",
        "    ['ollama', 'serve'], env=env,\n",
        "    stdout=log_file, stderr=subprocess.STDOUT,\n",
        ")\n",
        "\n",
        "def tail_log(path, stop):\n",
        "    with open(path, 'r') as f:\n",
        "        while not stop.is_set():\n",
        "            line = f.readline()\n",
        "            if line: print(line, end='', flush=True)\n",
        "            else: time.sleep(0.3)\n",
        "\n",
        "_stop_tail = threading.Event()\n",
        "threading.Thread(target=tail_log, args=('/content/ollama_server.log', _stop_tail), daemon=True).start()\n",
        "\n",
        "for i in range(30):\n",
        "    try:\n",
        "        requests.get('http://localhost:11434/', timeout=2)\n",
        "        print(f'\\nOllama ready ({i+1}s)')\n",
        "        break\n",
        "    except requests.exceptions.RequestException:\n",
        "        time.sleep(1)\n",
        "else:\n",
        "    print('\\nOllama failed to start â€” check logs above')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pull_model"
      },
      "outputs": [],
      "source": [
        "#Pull Model\n",
        "MODEL = 'qwen3:32b'\n",
        "\n",
        "print(f'Pulling {MODEL}...')\n",
        "!OLLAMA_MODELS={MODELS_PATH} ollama pull {MODEL}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_verify"
      },
      "source": [
        "---\n",
        "## Verify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sanity_tests"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/MCP')\n",
        "!python -m tests.sanity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_benchmarks"
      },
      "source": [
        "---\n",
        "## Benchmarks\n",
        "\n",
        "A/B comparison: Baseline (no tools) vs ReasonForge (with tools)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "math_benchmark"
      },
      "outputs": [],
      "source": [
        "# MATH-500 Benchmark\n",
        "N_MATH = 50\n",
        "SKIP_BASELINE = False\n",
        "THINK = True\n",
        "\n",
        "os.chdir('/content/MCP')\n",
        "cmd = f'python -m tests.benchmark --model {MODEL} --n {N_MATH}'\n",
        "if SKIP_BASELINE: cmd += ' --skip-baseline'\n",
        "if THINK: cmd += ' --think'\n",
        "print(f'Running: {cmd}\\n')\n",
        "!{cmd}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_benchmark"
      },
      "outputs": [],
      "source": [
        "# HumanEval Code Benchmark\n",
        "N_CODE = 20\n",
        "SKIP_BASELINE_CODE = False\n",
        "THINK_CODE = True\n",
        "\n",
        "os.chdir('/content/MCP')\n",
        "cmd = f'python -m tests.code_benchmark --model {MODEL} --n {N_CODE}'\n",
        "if SKIP_BASELINE_CODE: cmd += ' --skip-baseline'\n",
        "if THINK_CODE: cmd += ' --think'\n",
        "print(f'Running: {cmd}\\n')\n",
        "!{cmd}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_ui"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "launch_ui"
      },
      "outputs": [],
      "source": [
        "os.environ['RF_SHARE'] = '1'\n",
        "os.chdir('/content/MCP')\n",
        "!python -m ui.app"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "view_logs"
      },
      "outputs": [],
      "source": [
        "# !tail -50 /content/ollama_server.log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ollama_status"
      },
      "outputs": [],
      "source": [
        "# !OLLAMA_MODELS=$MODELS_PATH ollama ps\n",
        "# print()\n",
        "# !OLLAMA_MODELS=$MODELS_PATH ollama list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup"
      },
      "outputs": [],
      "source": [
        "# !rm -rf /content/MCP\n",
        "# !rm /content/ollama_server.log"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "G4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
