{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# ReasonForge\n",
        "\n",
        "Deterministic math & code tools for small language models.\n",
        "\n",
        "1. Clone repo & install deps\n",
        "2. Install Ollama & pull model\n",
        "3. Sanity tests\n",
        "4. MATH-500 / HumanEval benchmarks\n",
        "5. Gradio chat UI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_setup"
      },
      "source": [
        "---\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone_and_install"
      },
      "outputs": [],
      "source": [
        "# Clone repo & install deps\n",
        "!git clone https://github.com/RoyCoding8/MCP.git /content/MCP\n",
        "!pip uninstall -y -q omegaconf\n",
        "!pip install -q \"gradio>=6.0\" sympy datasets \"math-verify[antlr4_13_2]\" pandas matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_ollama"
      },
      "outputs": [],
      "source": [
        "# Install Ollama\n",
        "!sudo apt-get install -qq zstd\n",
        "!curl -fsSL https://ollama.com/install.sh | sh > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "start_ollama"
      },
      "outputs": [],
      "source": [
        "import subprocess, time, os\n",
        "import requests\n",
        "\n",
        "MODELS_PATH = '/content/ollama_models'\n",
        "os.makedirs(MODELS_PATH, exist_ok=True)\n",
        "print(f'Models path: {MODELS_PATH}')\n",
        "\n",
        "subprocess.run(['pkill', '-f', 'ollama'], capture_output=True)\n",
        "time.sleep(2)\n",
        "\n",
        "env = os.environ.copy()\n",
        "env['OLLAMA_MODELS'] = MODELS_PATH\n",
        "env['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "env['OLLAMA_NUM_PARALLEL'] = '15'\n",
        "\n",
        "print(\"Starting Ollama server...\")\n",
        "log_file = open('/content/ollama_server.log', 'w')\n",
        "proc = subprocess.Popen(\n",
        "    ['ollama', 'serve'],\n",
        "    env=env,\n",
        "    stdout=log_file,\n",
        "    stderr=subprocess.STDOUT,\n",
        ")\n",
        "\n",
        "start = time.time()\n",
        "while time.time()-start < 30:\n",
        "    try:\n",
        "        requests.get('http://localhost:11434/')\n",
        "        print(f'Ollama ready ({time.time()-start:.2f}s)')\n",
        "        break\n",
        "    except requests.ConnectionError:\n",
        "        time.sleep(1)\n",
        "else:\n",
        "    print('Ollama failed to start — check /content/ollama_server.log')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pull_model"
      },
      "outputs": [],
      "source": [
        "# Pull Models\n",
        "MODELS = ['qwen3:8b','qwen3:32b']\n",
        "\n",
        "for MODEL in MODELS:\n",
        "    print(f'Pulling {MODEL}...')\n",
        "    !OLLAMA_MODELS={MODELS_PATH} ollama pull {MODEL}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_verify"
      },
      "source": [
        "---\n",
        "## Verify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sanity_tests"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/MCP')\n",
        "!python -m tests.sanity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_benchmarks"
      },
      "source": [
        "---\n",
        "## Benchmarks\n",
        "\n",
        "A/B comparison: Baseline (no tools) vs ReasonForge (with tools)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "math_benchmark"
      },
      "outputs": [],
      "source": [
        "# MATH-500 Benchmark\n",
        "N_MATH = 5\n",
        "SKIP_BASELINE = False\n",
        "THINK = True\n",
        "\n",
        "from google.colab import userdata\n",
        "try: os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n",
        "except: print(\"HF_TOKEN not found in Secrets.\")\n",
        "\n",
        "os.chdir('/content/MCP')\n",
        "cmd = f'python -m tests.benchmark --model {MODELS[0]} --n {N_MATH}'\n",
        "if SKIP_BASELINE: cmd += ' --skip-baseline'\n",
        "if THINK: cmd += ' --think'\n",
        "print(f'Running: {cmd}\\n')\n",
        "!{cmd}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "math_viz"
      },
      "outputs": [],
      "source": [
        "# Visualize MATH-500 results\n",
        "import pandas as pd, matplotlib.pyplot as plt, json as _json\n",
        "from pathlib import Path\n",
        "\n",
        "results_dir = Path('/content/MCP/tests/results')\n",
        "files = sorted(results_dir.glob('*.json'), key=lambda f: f.stat().st_mtime)\n",
        "if not files:\n",
        "    print('No results found. Run the benchmark first.')\n",
        "else:\n",
        "    latest = files[-1]\n",
        "    print(f'Loading: {latest.name}\\n')\n",
        "    with open(latest) as f: report = _json.load(f)\n",
        "\n",
        "    df = pd.DataFrame(report['results'])\n",
        "    print(f\"Model: {report['model']}  |  N={report['n']}  |  Seed={report['seed']}\")\n",
        "    print(f\"RF Accuracy: {report['rf_accuracy']:.1%}\")\n",
        "    if report.get('baseline_accuracy') is not None:\n",
        "        print(f\"Baseline Accuracy: {report['baseline_accuracy']:.1%}\")\n",
        "        print(f\"Delta: {report['delta']:+.1%}\")\n",
        "    print()\n",
        "\n",
        "    display(df[['type','level','expected','baseline_answer','baseline_correct','rf_answer','rf_correct','rf_rounds','rf_used_tools','weight']])\n",
        "\n",
        "    # By difficulty\n",
        "    if 'baseline_correct' in df.columns and df['baseline_correct'].any():\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "        lvl = df.groupby('level').agg(Baseline=('baseline_correct','mean'), ReasonForge=('rf_correct','mean')).sort_index()\n",
        "        lvl.plot.bar(ax=axes[0], rot=0, color=['#94a3b8','#3b82f6'])\n",
        "        axes[0].set_title('Accuracy by Difficulty')\n",
        "        axes[0].set_ylabel('Accuracy')\n",
        "        axes[0].set_ylim(0, 1.05)\n",
        "        axes[0].legend(loc='upper right')\n",
        "\n",
        "        cat = df.groupby('type').agg(Baseline=('baseline_correct','mean'), ReasonForge=('rf_correct','mean')).sort_index()\n",
        "        cat.plot.bar(ax=axes[1], rot=30, color=['#94a3b8','#3b82f6'])\n",
        "        axes[1].set_title('Accuracy by Category')\n",
        "        axes[1].set_ylabel('Accuracy')\n",
        "        axes[1].set_ylim(0, 1.05)\n",
        "        axes[1].legend(loc='upper right')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('/content/MCP/tests/results/math_results.png', dpi=150, bbox_inches='tight')\n",
        "        plt.show()\n",
        "    else:\n",
        "        fig, ax = plt.subplots(figsize=(7, 5))\n",
        "        lvl = df.groupby('level')['rf_correct'].mean().sort_index()\n",
        "        lvl.plot.bar(ax=ax, rot=0, color='#3b82f6')\n",
        "        ax.set_title('ReasonForge Accuracy by Difficulty')\n",
        "        ax.set_ylabel('Accuracy')\n",
        "        ax.set_ylim(0, 1.05)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('/content/MCP/tests/results/math_results.png', dpi=150, bbox_inches='tight')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_benchmark"
      },
      "outputs": [],
      "source": [
        "# HumanEval Code Benchmark\n",
        "N_CODE = 2\n",
        "SKIP_BASELINE_CODE = False\n",
        "THINK_CODE = True\n",
        "SEED = 42\n",
        "\n",
        "os.chdir('/content/MCP')\n",
        "cmd = f'python -m tests.code_benchmark --model {MODEL} --n {N_CODE} --seed {SEED}'\n",
        "if SKIP_BASELINE_CODE: cmd += ' --skip-baseline'\n",
        "if THINK_CODE: cmd += ' --think'\n",
        "print(f'Running: {cmd}\\n')\n",
        "!{cmd}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_viz"
      },
      "outputs": [],
      "source": [
        "# Visualize HumanEval results\n",
        "results_dir = Path('/content/MCP/tests/results')\n",
        "files = sorted(results_dir.glob('code_*.json'), key=lambda f: f.stat().st_mtime)\n",
        "if not files:\n",
        "    print('No code benchmark results found. Run the benchmark first.')\n",
        "else:\n",
        "    latest = files[-1]\n",
        "    print(f'Loading: {latest.name}\\n')\n",
        "    with open(latest) as f: report = _json.load(f)\n",
        "\n",
        "    df = pd.DataFrame(report['results'])\n",
        "    print(f\"Model: {report['model']}  |  N={report['n']}  |  Seed={report['seed']}\")\n",
        "    print(f\"RF Pass@1: {report['rf_pass1']:.1%}\")\n",
        "    if report.get('baseline_pass1') is not None:\n",
        "        print(f\"Baseline Pass@1: {report['baseline_pass1']:.1%}\")\n",
        "        print(f\"Delta: {report['delta']:+.1%}\")\n",
        "    print()\n",
        "\n",
        "    display(df)\n",
        "\n",
        "    # Summary bar chart\n",
        "    fig, ax = plt.subplots(figsize=(6, 4))\n",
        "    labels = ['ReasonForge']\n",
        "    vals = [report['rf_pass1']]\n",
        "    colors = ['#3b82f6']\n",
        "    if report.get('baseline_pass1') is not None:\n",
        "        labels.insert(0, 'Baseline')\n",
        "        vals.insert(0, report['baseline_pass1'])\n",
        "        colors.insert(0, '#94a3b8')\n",
        "    bars = ax.bar(labels, vals, color=colors, width=0.5)\n",
        "    for bar, v in zip(bars, vals):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, f'{v:.0%}', ha='center', fontweight='bold')\n",
        "    ax.set_ylim(0, 1.15)\n",
        "    ax.set_ylabel('Pass Rate')\n",
        "    ax.set_title(f'HumanEval � {report[\"model\"]}')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/MCP/tests/results/code_results.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_ui"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "launch_ui"
      },
      "outputs": [],
      "source": [
        "os.environ['RF_SHARE'] = '1'\n",
        "os.chdir('/content/MCP')\n",
        "\n",
        "!python -u -m ui.app | tee -a /content/ollama_server.log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9dsarqNmgUN"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ollama_status"
      },
      "outputs": [],
      "source": [
        "# !OLLAMA_MODELS=$MODELS_PATH ollama ps\n",
        "# print()\n",
        "# !OLLAMA_MODELS=$MODELS_PATH ollama list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup"
      },
      "outputs": [],
      "source": [
        "# !rm -rf /content/MCP\n",
        "# !rm /content/ollama_server.log"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "G4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
