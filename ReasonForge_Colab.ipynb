{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# ReasonForge\n",
        "\n",
        "Deterministic math & code tools for small language models.\n",
        "\n",
        "1. Clone repo & install deps\n",
        "2. Install Ollama & pull model\n",
        "3. Sanity tests\n",
        "4. MATH-500 / HumanEval benchmarks\n",
        "5. Gradio chat UI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_setup"
      },
      "source": [
        "---\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone_and_install"
      },
      "outputs": [],
      "source": [
        "# Clone repo & install deps\n",
        "!git clone https://github.com/RoyCoding8/MCP.git /content/MCP\n",
        "!pip install -q gradio>=6.0 sympy datasets math-verify[antlr4_13_2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_ollama"
      },
      "outputs": [],
      "source": [
        "# Install Ollama\n",
        "!sudo apt-get install -qq zstd > /dev/null 2>&1\n",
        "!curl -fsSL https://ollama.com/install.sh | sh > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mount_drive",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4f96d00-94e1-4a5a-9fe3-d4d25d574dfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models path: /content/ollama_models\n"
          ]
        }
      ],
      "source": [
        "# Store models locally in the Colab VM\n",
        "import os\n",
        "\n",
        "MODELS_PATH = '/content/ollama_models'\n",
        "os.makedirs(MODELS_PATH, exist_ok=True)\n",
        "print(f'Models path: {MODELS_PATH}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "start_ollama"
      },
      "outputs": [],
      "source": [
        "import subprocess, time, os\n",
        "import requests\n",
        "\n",
        "subprocess.run(['pkill', '-f', 'ollama'], capture_output=True)\n",
        "time.sleep(2)\n",
        "\n",
        "env = os.environ.copy()\n",
        "env['OLLAMA_MODELS'] = MODELS_PATH\n",
        "env['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "\n",
        "print(\"Starting Ollama server...\")\n",
        "log_file = open('/content/ollama_server.log', 'w')\n",
        "proc = subprocess.Popen(\n",
        "    ['ollama', 'serve'],\n",
        "    env=env,\n",
        "    stdout=log_file,\n",
        "    stderr=subprocess.STDOUT,\n",
        ")\n",
        "\n",
        "start = time.time()\n",
        "while time.time()-start < 30:\n",
        "    try:\n",
        "        requests.get('http://localhost:11434/')\n",
        "        print(f'Ollama ready ({time.time()-start:.2f}s)')\n",
        "        break\n",
        "    except requests.ConnectionError:\n",
        "        time.sleep(1)\n",
        "else:\n",
        "    print('Ollama failed to start â€” check /content/ollama_server.log')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "pull_model",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9762058d-d270-4859-8319-78ced88b474a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pulling qwen3:8b...\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ],
      "source": [
        "# Pull Models\n",
        "MODELS = ['qwen3:8b']\n",
        "\n",
        "for MODEL in MODELS:\n",
        "    print(f'Pulling {MODEL}...')\n",
        "    !OLLAMA_MODELS={MODELS_PATH} ollama pull {MODEL}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_verify"
      },
      "source": [
        "---\n",
        "## Verify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sanity_tests"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/MCP')\n",
        "!python -m tests.sanity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_benchmarks"
      },
      "source": [
        "---\n",
        "## Benchmarks\n",
        "\n",
        "A/B comparison: Baseline (no tools) vs ReasonForge (with tools)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "math_benchmark"
      },
      "outputs": [],
      "source": [
        "# MATH-500 Benchmark\n",
        "N_MATH = 1\n",
        "SKIP_BASELINE = False\n",
        "THINK = False\n",
        "\n",
        "import subprocess\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "try: os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n",
        "except: print(\"HF_TOKEN not found in Secrets. Proceeding without authentication.\")\n",
        "\n",
        "# Construct command list\n",
        "cmd = ['python', '-u', '-m', 'tests.benchmark', '--model', MODEL, '--n', str(N_MATH)]\n",
        "if SKIP_BASELINE: cmd.append('--skip-baseline')\n",
        "if THINK: cmd.append('--think')\n",
        "\n",
        "print(f\"Running: {' '.join(cmd)}\\n\")\n",
        "\n",
        "with open('/content/ollama_server.log', 'a') as log:\n",
        "    process = subprocess.Popen(\n",
        "        cmd,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True,\n",
        "        bufsize=1,\n",
        "        cwd='/content/MCP'\n",
        "    )\n",
        "    for line in process.stdout:\n",
        "        log.write(line)\n",
        "        log.flush()\n",
        "    process.wait()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_benchmark"
      },
      "outputs": [],
      "source": [
        "# HumanEval Code Benchmark\n",
        "N_CODE = 1\n",
        "SKIP_BASELINE_CODE = False\n",
        "THINK_CODE = True\n",
        "\n",
        "cmd = ['python', '-u', '-m', 'tests.code_benchmark', '--model', MODEL, '--n', str(N_CODE)]\n",
        "if SKIP_BASELINE_CODE: cmd.append('--skip-baseline')\n",
        "if THINK_CODE: cmd.append('--think')\n",
        "\n",
        "print(f\"Running: {' '.join(cmd)}\\n\")\n",
        "\n",
        "with open('/content/ollama_server.log', 'a') as log:\n",
        "    process = subprocess.Popen(\n",
        "        cmd,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True,\n",
        "        bufsize=1,\n",
        "        cwd='/content/MCP'\n",
        "    )\n",
        "    for line in process.stdout:\n",
        "        print(line, end='')\n",
        "        log.write(line)\n",
        "        log.flush()\n",
        "    process.wait()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_ui"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "launch_ui"
      },
      "outputs": [],
      "source": [
        "os.environ['RF_SHARE'] = '1'\n",
        "os.chdir('/content/MCP')\n",
        "\n",
        "!python -u -m ui.app | tee -a /content/ollama_server.log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9dsarqNmgUN"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ollama_status"
      },
      "outputs": [],
      "source": [
        "# !OLLAMA_MODELS=$MODELS_PATH ollama ps\n",
        "# print()\n",
        "# !OLLAMA_MODELS=$MODELS_PATH ollama list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup"
      },
      "outputs": [],
      "source": [
        "# !rm -rf /content/MCP\n",
        "# !rm /content/ollama_server.log"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "G4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}