{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# ReasonForge\n",
        "\n",
        "Deterministic math & code tools for small language models.\n",
        "\n",
        "1. Clone repo & install deps\n",
        "2. Install Ollama & pull model\n",
        "3. Sanity tests\n",
        "4. MATH-500 / HumanEval benchmarks\n",
        "5. Gradio chat UI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_setup"
      },
      "source": [
        "---\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone_and_install"
      },
      "outputs": [],
      "source": [
        "# Clone repo & install deps\n",
        "!git clone https://github.com/RoyCoding8/MCP.git /content/MCP\n",
        "!pip uninstall -y -q omegaconf\n",
        "!pip install -q \"gradio>=6.0\" sympy datasets \"math-verify[antlr4_13_2]\" pandas matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_ollama"
      },
      "outputs": [],
      "source": [
        "# Install Ollama\n",
        "!sudo apt-get install -qq zstd\n",
        "!curl -fsSL https://ollama.com/install.sh | sh > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount_drive"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "RESULTS_DIR = '/content/drive/MyDrive/ReasonForge/results'\n",
        "!mkdir -p {RESULTS_DIR}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess, time, os, requests\n",
        "from pathlib import Path\n",
        "import json as _json\n",
        "\n",
        "MODELS_PATH = '/content/ollama_models'\n",
        "os.makedirs(MODELS_PATH, exist_ok=True)\n",
        "\n",
        "# I am using G4 GPU with ~96GB VRAM, ajust values according to your GPU\n",
        "MODEL_CONFIG = {\n",
        "    'qwen3:8b':  15,\n",
        "    'qwen3:32b': 7,\n",
        "    # 'qwen3:4b':  30,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "helpers"
      },
      "outputs": [],
      "source": [
        "# Helper functions\n",
        "def start_ollama(num_parallel=None, timeout=30):\n",
        "    subprocess.run(['pkill', '-f', 'ollama'], capture_output=True)\n",
        "    time.sleep(2)\n",
        "\n",
        "    env = os.environ.copy()\n",
        "    env['OLLAMA_MODELS'] = MODELS_PATH\n",
        "    env['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "    if num_parallel is not None:\n",
        "        env['OLLAMA_NUM_PARALLEL'] = str(num_parallel)\n",
        "\n",
        "    log_file = open('/content/ollama_server.log', 'a')\n",
        "    subprocess.Popen(\n",
        "        ['ollama', 'serve'],\n",
        "        env=env,\n",
        "        stdout=log_file,\n",
        "        stderr=subprocess.STDOUT,\n",
        "    )\n",
        "\n",
        "    start = time.time()\n",
        "    while time.time() - start < timeout:\n",
        "        try:\n",
        "            requests.get('http://localhost:11434/')\n",
        "            print(f'Ollama ready ({time.time()-start:.2f}s)',\n",
        "                  f'  parallel={num_parallel}' if num_parallel else '')\n",
        "            return True\n",
        "        except requests.ConnectionError:\n",
        "            time.sleep(1)\n",
        "    print('Ollama failed to start â€” check /content/ollama_server.log')\n",
        "    return False\n",
        "\n",
        "\n",
        "def run_benchmark_suite(benchmark_module, n, skip_baseline, think, separator='-',seed=42):\n",
        "    os.chdir('/content/MCP')\n",
        "    for model, num_parallel in MODEL_CONFIG.items():\n",
        "        start_ollama(num_parallel)\n",
        "        print(f'\\n{separator*56}')\n",
        "        print(f'  {model}  (parallel={num_parallel})')\n",
        "        print(f'{separator*56}')\n",
        "        cmd = f'python -m {benchmark_module} --model {model} --n {n} --seed {seed}'\n",
        "        cmd += f' --results-dir {RESULTS_DIR}'\n",
        "        if skip_baseline: cmd += ' --skip-baseline'\n",
        "        if think:         cmd += ' --think'\n",
        "        !{cmd}\n",
        "\n",
        "\n",
        "def load_latest_result(glob_pattern='*.json'):\n",
        "    results_path = Path(RESULTS_DIR)\n",
        "    files = sorted(results_path.glob(glob_pattern), key=lambda f: f.stat().st_mtime)\n",
        "    if not files:\n",
        "        print(f'No results matching {glob_pattern!r}. Run the benchmark first.')\n",
        "        return None, None\n",
        "    latest = files[-1]\n",
        "    print(f'Loading: {latest.name}\\n')\n",
        "    with open(latest) as f:\n",
        "        report = _json.load(f)\n",
        "    return report, latest\n",
        "\n",
        "\n",
        "def print_report_header(report, accuracy_key='rf_accuracy', baseline_key='baseline_accuracy', delta_key='delta'):\n",
        "    print(f\"Model: {report['model']}  |  N={report['n']}  |  Seed={report['seed']}\")\n",
        "    print(f\"{accuracy_key.replace('_',' ').title()}: {report[accuracy_key]:.1%}\")\n",
        "    if report.get(baseline_key) is not None:\n",
        "        print(f\"{baseline_key.replace('_',' ').title()}: {report[baseline_key]:.1%}\")\n",
        "        print(f\"Delta: {report[delta_key]:+.1%}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "start_ollama"
      },
      "outputs": [],
      "source": [
        "start_ollama()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pull_model"
      },
      "outputs": [],
      "source": [
        "for model in MODEL_CONFIG:\n",
        "    print(f'Pulling {model}...')\n",
        "    !OLLAMA_MODELS={MODELS_PATH} ollama pull {model}\n",
        "print('All models pulled.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_verify"
      },
      "source": [
        "---\n",
        "## Verify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sanity_tests"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/MCP')\n",
        "!python -m tests.sanity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_benchmarks"
      },
      "source": [
        "---\n",
        "## Benchmarks\n",
        "\n",
        "A/B comparison: Baseline (no tools) vs ReasonForge (with tools).\n",
        "\n",
        "Results are checkpointed to Google Drive after each problem. If the notebook crashes, re-running will resume from where it left off."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "math_benchmark"
      },
      "outputs": [],
      "source": [
        "# MATH-500 Benchmark\n",
        "N_MATH = 50\n",
        "SKIP_BASELINE = False\n",
        "THINK = True\n",
        "\n",
        "from google.colab import userdata\n",
        "try: os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n",
        "except: print('HF_TOKEN not found in Secrets.')\n",
        "\n",
        "run_benchmark_suite('tests.math_benchmark', N_MATH, SKIP_BASELINE, THINK, separator='-',seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "math_viz"
      },
      "outputs": [],
      "source": [
        "# Visualize MATH-500 results\n",
        "import pandas as pd, matplotlib.pyplot as plt\n",
        "\n",
        "report, _ = load_latest_result('*.json')\n",
        "if report:\n",
        "    print_report_header(report, 'rf_accuracy', 'baseline_accuracy')\n",
        "    df = pd.DataFrame(report['results'])\n",
        "    display(df[['type','level','expected','baseline_answer','baseline_correct','rf_answer','rf_correct','rf_rounds','rf_used_tools','weight']])\n",
        "\n",
        "    # By difficulty\n",
        "    if 'baseline_correct' in df.columns and df['baseline_correct'].any():\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "        lvl = df.groupby('level').agg(Baseline=('baseline_correct','mean'), ReasonForge=('rf_correct','mean')).sort_index()\n",
        "        lvl.plot.bar(ax=axes[0], rot=0, color=['#94a3b8','#3b82f6'])\n",
        "        axes[0].set_title('Accuracy by Difficulty')\n",
        "        axes[0].set_ylabel('Accuracy')\n",
        "        axes[0].set_ylim(0, 1.05)\n",
        "        axes[0].legend(loc='upper right')\n",
        "\n",
        "        cat = df.groupby('type').agg(Baseline=('baseline_correct','mean'), ReasonForge=('rf_correct','mean')).sort_index()\n",
        "        cat.plot.bar(ax=axes[1], rot=30, color=['#94a3b8','#3b82f6'])\n",
        "        axes[1].set_title('Accuracy by Category')\n",
        "        axes[1].set_ylabel('Accuracy')\n",
        "        axes[1].set_ylim(0, 1.05)\n",
        "        axes[1].legend(loc='upper right')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{RESULTS_DIR}/math_results.png', dpi=150, bbox_inches='tight')\n",
        "        plt.show()\n",
        "    else:\n",
        "        fig, ax = plt.subplots(figsize=(7, 5))\n",
        "        lvl = df.groupby('level')['rf_correct'].mean().sort_index()\n",
        "        lvl.plot.bar(ax=ax, rot=0, color='#3b82f6')\n",
        "        ax.set_title('ReasonForge Accuracy by Difficulty')\n",
        "        ax.set_ylabel('Accuracy')\n",
        "        ax.set_ylim(0, 1.05)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{RESULTS_DIR}/math_results.png', dpi=150, bbox_inches='tight')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_benchmark"
      },
      "outputs": [],
      "source": [
        "# HumanEval Code Benchmark\n",
        "N_CODE = 20\n",
        "SKIP_BASELINE_CODE = False\n",
        "THINK_CODE = True\n",
        "\n",
        "run_benchmark_suite('tests.code_benchmark', N_CODE, SKIP_BASELINE_CODE, THINK_CODE, separator='=',seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_viz"
      },
      "outputs": [],
      "source": [
        "# Visualize HumanEval results\n",
        "import pandas as pd, matplotlib.pyplot as plt\n",
        "\n",
        "report, _ = load_latest_result('code_*.json')\n",
        "if report:\n",
        "    print_report_header(report, 'rf_pass1', 'baseline_pass1')\n",
        "    df = pd.DataFrame(report['results'])\n",
        "    display(df)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6, 4))\n",
        "    labels = ['ReasonForge']\n",
        "    vals = [report['rf_pass1']]\n",
        "    colors = ['#3b82f6']\n",
        "    if report.get('baseline_pass1') is not None:\n",
        "        labels.insert(0, 'Baseline')\n",
        "        vals.insert(0, report['baseline_pass1'])\n",
        "        colors.insert(0, '#94a3b8')\n",
        "    bars = ax.bar(labels, vals, color=colors, width=0.5)\n",
        "    for bar, v in zip(bars, vals):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, f'{v:.0%}', ha='center', fontweight='bold')\n",
        "    ax.set_ylim(0, 1.15)\n",
        "    ax.set_ylabel('Pass Rate')\n",
        "    ax.set_title(f'HumanEval {report[\"model\"]}')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{RESULTS_DIR}/code_results.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_ui"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "launch_ui"
      },
      "outputs": [],
      "source": [
        "os.environ['RF_SHARE'] = '1'\n",
        "os.chdir('/content/MCP')\n",
        "\n",
        "!python -u -m ui.app | tee -a /content/ollama_server.log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9dsarqNmgUN"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ollama_status"
      },
      "outputs": [],
      "source": [
        "# !OLLAMA_MODELS=$MODELS_PATH ollama ps\n",
        "# print()\n",
        "# !OLLAMA_MODELS=$MODELS_PATH ollama list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup"
      },
      "outputs": [],
      "source": [
        "# !rm -rf /content/MCP\n",
        "# !rm /content/ollama_server.log"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "G4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.14.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
